# Experiment information
experiment:
  # Custom indentifier name for the experiment.
  # Experiment
  name: EfficientNetB1-McDropout-C876
  notes: EfficientNet with MC-Dropout for Binary Classification.
  tags:
    - efficientnet
    - b1
    - C876
    - dropout_rate=0.5
    - catsvsdogs
    - binary
  # Experiments folder: Contains multiples experiments
  experiments_folder: $HER2BDL_EXPERIMENTS/runs
  # (Optional, default=null) numeric identfier for the experiment. If `null`, will be randomly picked.
  run_id: null
  # (Optional, default=null) Random seed, if it`s null, uses a random seed.
  seed: 1234

# Model architecture
model:
  # Model task: [classification|regression|hierarchical]
  task: classification
  # Model Class implemented in her2bdl.models
  architecture: EfficientNetMCDropout
  # (Optional, default=null) Models pretrained weights path. If it is null, use random weight initialization.
  weights: null
  # Model architecture hyperparameters
  hyperparameters: 
    mc_dropout_rate: 0.5
    base_model: B1
    efficient_net_weights: imagenet
    classifier_dense_layers: 
      - 256
      - 128
      - 64
  # Model uncertainty hyperparameters
  uncertainty:
    sample_size: 200
    mc_dropout_batch_size: 32
    multual_information: true
    variation_ratio: true
    predictive_entropy: true

# Aggregation methods
aggregation: 
  # Method Class
  method: null
  # Mathod parameters
  parameters: null

# Datasets and preprocessing
data:
  # Source
  source:
    type: tf_Dataset
    # Source type parameters
    parameters:
      dataset_target: binary
      data_dir: null
  validation_split: 0.1
  # Input
  img_height: 240
  img_width: 240
  img_channels: 3
  preprocessing:
    rescale: null
  # Target
  num_classes: 2
  label_mode: categorical  # Depends on task and model architecture
  labels: 
    - 'cat'
    - 'dog'

# Training model hyperparameters
training:
  # Training epochs
  epochs: 10
  # (Optional, default=16) Depends on CPU-GPU available memory.
  batch_size: 16
  # (Optional, default=0.2) Train and validation split.
  validation_split: 0.1
  # Models loss function
  loss:
    function: CategoricalCrossentropy
    parameters: null
  # (Optional, default=Adam(lr=1e-4)) Training optimizer
  optimizer:
    learning_rate: 1e-4
    name: rmsprop
    parameters: 
       centered: True
  # Training callbacks
  callbacks:
    # Connect to weight and bias *Requiere plugins and env variable.* 
    enable_wandb: true
    # Early stop, use null to disable. 
    earlystop: 
      patience: 4
      monitor: val_loss
    # Experiments results while training
    experiment_tracker:
      # Save model architecture summary
      #log_model_summary: true
      # Save datasets info: source, sizes, etc. 
      #log_dataset_description: true
      # Plots and logs
      #log_training_loss: true
      log_predictions: true
      log_uncertainty: true
      log_confusion_matrix: true
      log_metrics: true
      log_roc_curve: true
    # Save checkpoints: saved at experiments_folder/experiment/checkpoints
    checkpoints:
      # saved weights format
      format: "weights.{epoch:03d}-{val_loss:.4f}.h5"
      save_best_only: true
      save_weights_only: true
      monitor: val_loss
# Evaluation metrics
evaluation:
  evaluate_classification: true
  evaluate_aleatoric_uncertainty: false
  evaluate_aggregation: false
# Predict 
predict:
  save_aggregation: false
  save_predictions: true
  save_uncertainty: false
# Plugin setups
plugins:
  wandb:
    project: Her2BDL
    # Use environment variable name (recommended) or API KEY.
    apikey: $HER2BDL_EXPERIMENTS/.wandb_secret