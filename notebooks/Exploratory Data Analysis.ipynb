{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><a href=\"https://github.com/sborquez/her2bdl\"> Her2BDL</a> - Her2 Bayesian Deep Learning</h1>\n",
    "\n",
    "<br>\n",
    "<img src=\"images/utfsm.png\" width=\"50%\"/>\n",
    "\n",
    "<h2 align=\"center\">Exploratory Data Analysis</h2>\n",
    "\n",
    "<center>\n",
    "<i> Notebook created by Sebastián Bórquez G. - <a href=\"mailto://sebstian.borquez@sansano.usm.cl\">sebastian.borquez@sansano.usm.cl</a> - utfsm - Agosto 2020.</i>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Option A) Colab Setup\n",
    "\n",
    "Connectar a tu `Google Drive` e instalar dependencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"/content/drive/<Path to Project>\"\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Opción B) Local Setup\n",
    "\n",
    "Cambiarse al directorio raíz del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar Modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her2BDL packege\n",
    "from her2bdl import *\n",
    "\n",
    "# Adhoc modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.core.display import display, HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Descripción y exploración del datasets, distribución de clases y ejemplos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warwick Her2 Scoring contest\n",
    "\n",
    "### Descripción\n",
    "\n",
    "WSIs are generally high resolution gigapixel images obtained by scanning the conventional glass slides. They are normally stored in pyramid structures containing several levels, each level has a different resolution. For visualization, the region-of-interest (ROI) from these images require specially designed libraries or tools. OpenSlide is one of the commonly used libraries that provides a simple interface to read WSIs.\n",
    "\n",
    "### Source\n",
    "\n",
    "- https://warwick.ac.uk/fac/sci/dcs/research/tia/her2contest/download/\n",
    "\n",
    "- Qaiser, Talha, et al. \"Her 2 challenge contest: a detailed assessment of automated her 2 scoring algorithms in whole slide images of breast cancer tissues.\" Histopathology 72.2 (2018): 227-238.\n",
    "https://onlinelibrary.wiley.com/doi/epdf/10.1111/his.13333\n",
    "\n",
    "- T. Qaiser, N.M. Rajpoot, \"Learning Where to See: A Novel Attention Model for Automated Immunohistochemical Scoring\", in IEEE Transactions on Medical Imaging, 2019. DOI: 10.1109/TMI.2019.2907049\n",
    "https://ieeexplore.ieee.org/document/8672928\n",
    "\n",
    "\n",
    "### Warwick Training Dataset\n",
    "\n",
    "The training dataset consists of 52 WSIs with equally distributed cases for all 4 possible stages of HER2 scoring (0/ 1+/2+/3+).\n",
    "\n",
    "The ground truth data for WSIs is provided in a spreadsheet containing the case number, HER2 score and percentage cells with complete membrane staining irrespective of intensity respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source = 'E:/datasets/medical/warwick/train'\n",
    "\n",
    "train_dataset = get_dataset(source, include_ground_truth=True)\n",
    "describe_dataset(train_dataset)\n",
    "train_dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warwick Testing Dataset \n",
    "The testing dataset contains 28 whole-slide-images (WSIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source = 'E:/datasets/medical/warwick/test'\n",
    "\n",
    "test_dataset = get_dataset(source, include_ground_truth=False)\n",
    "describe_dataset(test_dataset, include_targets=False)\n",
    "test_dataset.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test Splits\n",
    "\n",
    "Training dataset is divided into train/test 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = aggregate_dataset(load_dataset(\"./train/datasets/train.csv\"))\n",
    "test = aggregate_dataset(load_dataset(\"./train/datasets/test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_class_distribution(train, target=TARGET, target_labels=TARGET_LABELS, dataset_name=\"Training\")\n",
    "display_class_distribution(test, target=TARGET, target_labels=TARGET_LABELS, dataset_name=\"Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossValidation Splits\n",
    "\n",
    "Training set is splitted into K-folds for hyperparameters optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_splits = prepare_cv_splits(train, 5, seed=42)\n",
    "for i, (tr, ts) in enumerate(cv_splits, start=1):\n",
    "    display_class_distribution(tr, target=TARGET, target_labels=TARGET_LABELS, dataset_name=f\"Training - Hold out {i}\")\n",
    "    display_class_distribution(ts, target=TARGET, target_labels=TARGET_LABELS, dataset_name=f\"Validation - Hold out {i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Validation for Best parameters\n",
    "\n",
    "The model with best performance in CV will be retrained with a new split of the training set. The divisiion of training/validation with a 90/10 ratio will be used for earlystop and identify overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2, val_2 = split_dataset(dataset, validation_ratio=0.15, seed=42)\n",
    "display_class_distribution(train_2, target=TARGET, target_labels=TARGET_LABELS, dataset_name=f\"Training - Best Parameters\")\n",
    "display_class_distribution(val_2, target=TARGET, target_labels=TARGET_LABELS, dataset_name=f\"Validation - Best Parameters\");\n",
    "#save_dataset(train_2, output_folder=\"./train/datasets/best_parameters\", dataset_name=f\"training\")\n",
    "#save_dataset(val_2, output_folder=\"./train/datasets/best_parameters\", dataset_name=f\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSI: Sample images\n",
    "\n",
    "A whole-slide image is a digital representation of a microscopic slide, typically at a very high level of magnification such as 20x or 40x. As a result of this high magnification, whole slide images are typically very large in size. The maximum file size for a single whole-slide image in our training data set is 3.4 GB, with an average over 1 GB. [[source](https://developer.ibm.com/articles/an-automatic-method-to-identify-tissues-from-big-whole-slide-images-pt1/)]\n",
    "\n",
    "We can use the [OpenSlide](https://openslide.org/api/python) project to read a variety of whole-slide image formats. This is a pyramidal, tiled format, where the massive slide is composed of a large number of constituent tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = train_dataset.sample(5)\n",
    "wsi_images = []\n",
    "for _,sample in samples.iterrows():\n",
    "    wsi_images.append(open_wsi(sample[\"source\"], sample[\"CaseNo\"],sample[\"image_her2\"]))\n",
    "her2_wsi = open_wsi(sample[\"source\"], sample[\"CaseNo\"],sample[\"image_her2\"])\n",
    "describe_wsi(her2_wsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for her2_wsi in wsi_images:\n",
    "    size = her2_wsi.level_dimensions[-1]\n",
    "    display(size)\n",
    "    display(her2_wsi.get_thumbnail(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_wsi_sizes_distribution(train_dataset, \"Train Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = her2_wsi.level_dimensions[5]\n",
    "image = cv2.cvtColor(np.array(her2_wsi.get_thumbnail(size)),  cv2.COLOR_RGB2BGR)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# Otsu's thresholding\n",
    "ret2,th2 = cv2.threshold(gray,0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(image[:,:,::-1], interpolation=None)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(gray, interpolation=None, cmap=\"gray\")\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(th2, interpolation=None, cmap=\"binary\")\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTSU with HSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = her2_wsi.level_dimensions[5]\n",
    "image = cv2.cvtColor(np.array(her2_wsi.get_thumbnail(size)),  cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Otsu's thresholding\n",
    "ret2,th2 = cv2.threshold(gray,0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(img, interpolation=None)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generator\n",
    "\n",
    "Los generadores se encargan de alimentar a los modelos, generando los patches de imágenes etiquetados durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = aggregate_dataset(load_dataset(\"./train/datasets/train.csv\"))\n",
    "test = aggregate_dataset(load_dataset(\"./train/datasets/test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Generator\n",
    "\n",
    "Extract patches from a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = GridPatchGenerator(train, 1, 3, (224, 224))\n",
    "X_batch, y_batch = train_generator[0]\n",
    "for xi, yi in zip(X_batch, y_batch):\n",
    "    plot_sample(xi, yi.argmax())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = GridPatchGenerator(test, 2, 2, (224, 224))\n",
    "X_batch, y_batch = test_generator[0]\n",
    "for xi, yi in zip(X_batch, y_batch):\n",
    "    plot_sample(xi, yi.argmax())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCPatchGenerator\n",
    "\n",
    "Random patch samples from wsi image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator = MCPatchGenerator(train, 2, 3, (224, 224))\n",
    "X_batch, y_batch = train_generator[0]\n",
    "for xi, yi in zip(X_batch, y_batch):\n",
    "    plot_sample(xi, yi.argmax())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_generator = MCPatchGenerator(test, 2, 3, (224, 224))\n",
    "X_batch, y_batch = test_generator[3]\n",
    "for xi, yi in zip(X_batch, y_batch):\n",
    "    plot_sample(xi, yi.argmax())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
