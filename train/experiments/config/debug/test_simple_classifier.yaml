# Experiment information
experiment:
  # Custom indentifier name for the experiment.
  # Experiment
  name: testing_model
  notes: simple test for trainig a classifier.
  tags: 
    - test
    - simple
  # Experiments folder: Contains multiples experiments
  experiments_folder: D:/sebas/Google Drive/Projects/her2bdl/train/experiments/runs
  # experiments_folder: /data/atlas/dbetalhc/cta-test/gerumo/src/her2bdl/train/experiments/runs
  # (Optional, default=null) numeric identfier for the experiment. If `null`, will be randomly picked.
  run_id: null
  # (Optional, default=null) Random seed, if it`s null, uses a random seed.
  seed: 1234

# Model architecture
model:
  # Model task: [classification|regression|hierarchical]
  task: classification
  # Model Class implemented in her2bdl.models
  architecture: SimpleClassifierMCDropout
  # (Optional, default=null) Models pretrained weights path. If it is null, use random weight initialization.
  weights: null
  # Model architecture hyperparameters
  hyperparameters: 
    mc_dropout_rate: 0.5
  # Model uncertainty hyperparameters
  uncertainty:
    sample_size: 100
    mc_dropout_batch_size: 16
    multual_information: true
    variation_ratio: true
    predictive_entropy: true

# Aggregation methods
aggregation: 
  # Method Class
  method: null
  # Mathod parameters
  parameters: null

# Datasets and preprocessing
data:
  # Source
  source:
    type: tf_Dataset
    # Source type parameters
    parameters:
      dataset_target: simple
      data_dir: null
  # Input
  img_height: 224
  img_width: 224
  img_channels: 3
  preprocessing: 
    rescale: 0.00392156862745098
  # Target
  num_classes: 10
  label_mode: categorical  # Depends on task and model architecture
  labels: 
    - '0'
    - '1'
    - '2'
    - '3'
    - '4'
    - '5'
    - '6'
    - '7'
    - '8'
    - '9'

# Training model hyperparameters
training:
  # Training epochs
  epochs: 5
  # (Optional, default=16) Depends on CPU-GPU available memory.
  batch_size: 8 
  # (Optional, default=0.2) Train and validation split.
  validation_split: "sample"
  # Models loss function
  loss:
    function: CategoricalCrossentropy
    parameters: null
  # (Optional, default=Adam(lr=1e-4)) Training optimizer
  optimizer:
    name: adam
    learning_rate: 1e-4
    parameters: null
  # Training callbacks
  callbacks:
    # Connect to weight and bias *Requiere plugins and env variable.* 
    enable_wandb: true
    # Early stop, use null to disable. 
    earlystop: 
      patience: 15
      monitor: val_loss
    # Experiments results while training
    experiment_tracker:
      # Save model architecture summary
      log_model_summary: true
      # Save datasets info: source, sizes, etc. 
      log_dataset_description: true
      # Plots and logs
      log_training_loss: true
      log_predictions: false
      log_uncertainty: false
      log_metrics: true
      log_roc_curve: true
    # Save checkpoints: saved at experiments_folder/experiment/checkpoints
    checkpoints:
      # saved weights format
      format: "weights.{epoch:03d}-{val_loss:.4f}.h5"
      save_best_only: true
      save_weights_only: true
      monitor: val_loss
# Evaluation metrics
evaluate:
  metrics: [accuracy]
# Predict 
predict:
  save_aggregation: false
  save_predictions: true
  save_uncertainty: false
# Plugin setups
plugins:
  wandb:
    project: Her2BDL
    # Use environment variable name (recommended) or API KEY.
    apikey: WANDB_API_KEY